{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Параллельные вычисления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Автор задач: Блохин Н.В. (NVBlokhin@fa.ru)__\n",
    "\n",
    "Материалы:\n",
    "* Макрушин С.В. Лекция \"Параллельные вычисления\"\n",
    "* https://docs.python.org/3/library/multiprocessing.html\n",
    "    * https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Process\n",
    "    * https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool\n",
    "    * https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
    "* https://numpy.org/doc/stable/reference/generated/numpy.array_split.html\n",
    "* https://nalepae.github.io/pandarallel/\n",
    "    * https://github.com/nalepae/pandarallel/blob/master/docs/examples_windows.ipynb\n",
    "    * https://github.com/nalepae/pandarallel/blob/master/docs/examples_mac_linux.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandarallel\n",
      "  Using cached pandarallel-1.6.3-py3-none-any.whl\n",
      "Requirement already satisfied: psutil in d:\\anaconda\\envs\\python\\lib\\site-packages (from pandarallel) (5.9.0)\n",
      "Collecting dill>=0.3.1\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: pandas>=1 in c:\\users\\артём\\appdata\\roaming\\python\\python38\\site-packages (from pandarallel) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\anaconda\\envs\\python\\lib\\site-packages (from pandas>=1->pandarallel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\envs\\python\\lib\\site-packages (from pandas>=1->pandarallel) (2022.2.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\артём\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=1->pandarallel) (1.23.4)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\python\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1->pandarallel) (1.16.0)\n",
      "Installing collected packages: dill, pandarallel\n",
      "Successfully installed dill-0.3.6 pandarallel-1.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandarallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Посчитайте, сколько раз встречается буква \"a\" в файлах [\"xaa\", \"xab\", \"xac\", \"xad\"]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "files = [f\"10_multiprocessing_data/{name}.txt\" for name in [\"xaa\", \"xab\", \"xac\", \"xad\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_a(file):\n",
    "    with open(file) as fp:\n",
    "        text = fp.read().lower()\n",
    "    res = Counter(text)[\"a\"]\n",
    "    print(file, res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_multiprocessing_data/xaa.txt 2599627\n",
      "10_multiprocessing_data/xab.txt 2605911\n",
      "10_multiprocessing_data/xac.txt 2599868\n",
      "10_multiprocessing_data/xad.txt 1460452\n",
      "CPU times: total: 15.5 s\n",
      "Wall time: 15.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2599627, 2605911, 2599868, 1460452]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "[count_a(f) for f in files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting count_a.py\n"
     ]
    }
   ],
   "source": [
    "%%file count_a.py\n",
    "from collections import Counter\n",
    "\n",
    "def count_a(file):\n",
    "    with open(file) as fp:\n",
    "        text = fp.read().lower()\n",
    "    res = Counter(text)[\"a\"]\n",
    "    print(file, res)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from count_a import count_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 5.21 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2599627, 2605911, 2599868, 1460452]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "with multiprocessing.Pool(processes=4) as pool:\n",
    "    res = pool.map(count_a, files)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting count_a_q.py\n"
     ]
    }
   ],
   "source": [
    "%%file count_a_q.py\n",
    "from collections import Counter\n",
    "\n",
    "def count_a_q(file, queue):\n",
    "    with open(file) as fp:\n",
    "        text = fp.read().lower()\n",
    "    res = Counter(text)[\"a\"]\n",
    "    print(file, res)\n",
    "    queue.put(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from count_a_q import count_a_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.05 s\n",
      "Wall time: 5.07 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1460452, 2599868, 2605911, 2599627]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ps = []\n",
    "queue = multiprocessing.Queue()\n",
    "\n",
    "for f in files:\n",
    "    p = multiprocessing.Process(target=count_a_q, args=(f, queue))\n",
    "    ps.append(p)\n",
    "    p.start()\n",
    "\n",
    "rs = []\n",
    "while len(rs) < 4:\n",
    "    if not queue.empty():\n",
    "        rs.append(queue.get())\n",
    "    \n",
    "for p in ps:\n",
    "    p.join()\n",
    "\n",
    "rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Выведите на экран слова из файла words_alpha, в которых есть две или более буквы \"e\" подряд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "words = (\n",
    "    pd.read_csv(\"10_multiprocessing_data/words_alpha.txt\", header=None)[0]\n",
    "    .dropna()\n",
    "    .sample(frac=1, replace=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370103"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def f(s): \n",
    "    return bool(re.findall(r\"e{2,}\", s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting f.py\n"
     ]
    }
   ],
   "source": [
    "%%file f.py\n",
    "import re\n",
    "\n",
    "def f(s): \n",
    "    return bool(re.findall(r\"e{2,}\", s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandarallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "from f import f\n",
    "\n",
    "from pandarallel import pandarallel \n",
    "pandarallel.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 672 ms\n",
      "Wall time: 672 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7174"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "words[words.map(f)].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\Python\\lib\\site-packages\\pandarallel\\data_types\\series.py:42: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  yield data[chunk_]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 281 ms\n",
      "Wall time: 2.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7174"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "words[words.parallel_map(f)].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__При решении данных задач не подразумевается использования циклов или генераторов Python в ходе работы с пакетами `numpy` и `pandas`, если в задании не сказано обратного. Решения задач, в которых для обработки массивов `numpy` или структур `pandas` используются явные циклы (без согласования с преподавателем), могут быть признаны некорректными и не засчитаны.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. В каждой строке файла `tag_nsteps.csv` хранится информация о тэге рецепта и количестве шагов в этом рецепте в следующем виде:\n",
    "\n",
    "```\n",
    "tags,n_steps\n",
    "hungarian,2\n",
    "european,6\n",
    "occasion,4\n",
    "pumpkin,4\n",
    "................\n",
    "```\n",
    "\n",
    "Всего в исходном файле хранится чуть меньше, чем 71 млн, строк. Разбейте файл `tag_nsteps.csv` на несколько (например, 8) примерно одинаковых по объему файлов c названиями `tag_nsteps_*.csv`, где вместо символа `*` указан номер очередного файла. Каждый файл имеет структуру, аналогичную оригинальному файлу (включая заголовок).\n",
    "\n",
    "__Важно__: здесь и далее вы не можете загружать в память весь исходный файл сразу. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Артём\\\\OneDrive - ФГОБУ ВО Финансовый университет при Правительстве РФ\\\\Учёба\\\\3 курс\\\\Технологии обработки BD\\\\ТОБД22-ПМ20-Материалы к семинарам\\\\10_multiprocessing'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Всего строк в файле:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70695586\n"
     ]
    }
   ],
   "source": [
    "with open('C:\\\\Users\\\\Артём\\\\OneDrive - ФГОБУ ВО Финансовый университет при Правительстве РФ\\\\Учёба\\\\3 курс\\\\Технологии обработки BD\\\\ТОБД22-ПМ20-Материалы к семинарам\\\\10_multiprocessing\\\\10_multiprocessing_data\\\\tag_nsteps.csv'\n",
    ") as fp:\n",
    "    print(sum(1 for _ in fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Я решил разбить файл на 10 файлов. Девять файлов будут иметь `7069558` строк, а десятый `7069564` строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70695586"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7069558 * 9 + 7069564"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = pd.read_csv('C:\\\\Users\\\\Артём\\\\OneDrive - ФГОБУ ВО Финансовый университет при Правительстве РФ\\\\Учёба\\\\3 курс\\\\Технологии обработки BD\\\\ТОБД22-ПМ20-Материалы к семинарам\\\\10_multiprocessing\\\\10_multiprocessing_data\\\\tag_nsteps.csv', \n",
    "                     nrows=7069558,\n",
    "                     dtype={'tags': 'str', 'n_steps': np.int64})\n",
    "\n",
    "file_1.to_csv('tag_nsteps_1.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Со 2 по 9 файлы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 10):\n",
    "    file = pd.read_csv('C:\\\\Users\\\\Артём\\\\OneDrive - ФГОБУ ВО Финансовый университет при Правительстве РФ\\\\Учёба\\\\3 курс\\\\Технологии обработки BD\\\\ТОБД22-ПМ20-Материалы к семинарам\\\\10_multiprocessing\\\\10_multiprocessing_data\\\\tag_nsteps.csv', \n",
    "                        nrows=7069558, \n",
    "                        skiprows=np.arange(0, 7069558*(i-1) + 1),\n",
    "                        dtype={'tags': 'str', 'n_steps': np.int64},\n",
    "                        names=['tags', 'n_steps'])\n",
    "    file.to_csv(f'tag_nsteps_{i}.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_10 = pd.read_csv('C:\\\\Users\\\\Артём\\\\OneDrive - ФГОБУ ВО Финансовый университет при Правительстве РФ\\\\Учёба\\\\3 курс\\\\Технологии обработки BD\\\\ТОБД22-ПМ20-Материалы к семинарам\\\\10_multiprocessing\\\\10_multiprocessing_data\\\\tag_nsteps.csv', \n",
    "                     nrows=7069564,\n",
    "                     skiprows=np.arange(0, 7069558*9 + 1),\n",
    "                     dtype={'tags': 'str', 'n_steps': np.int64},\n",
    "                     names=['tags', 'n_steps'])\n",
    "\n",
    "file_10.to_csv(f'tag_nsteps_10.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Напишите функцию, которая принимает на вход название файла, созданного в результате решения задачи 1, считает для каждого тэга сумму по столбцу `n_steps` и количество строк c этим тэгом, и возвращает результат в виде словаря. Ожидаемый вид итогового словаря:\n",
    "\n",
    "```\n",
    "{\n",
    "    '1-day-or-more': {'sum': 56616, 'count': 12752},\n",
    "    '15-minutes-or-less': {'sum': 195413, 'count': 38898},\n",
    "    '3-steps-or-less': {'sum': 187938, 'count': 39711},\n",
    "    ....\n",
    "}\n",
    "```\n",
    "\n",
    "Примените данную функцию к каждому файлу, полученному в задании 1, и соберите результат в виде списка словарей. Не используйте параллельных вычислений. \n",
    "\n",
    "Выведите на экран значение по ключу \"30-minutes-or-less\" для каждого из словарей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_sum_count_from_file(file: str) -> dict:\n",
    "    df = pd.read_csv(file, dtype={'tags': 'str', 'n_steps': np.int64})\n",
    "    df = df.groupby('tags') \\\n",
    "        .agg({'n_steps':'sum', 'tags':'count'}) \\\n",
    "        .rename(columns={'n_steps':'sum','tags':'count'})\n",
    "    return df.T.to_dict('dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примените данную функцию к каждому файлу, полученному в задании 1, и соберите результат в виде списка словарей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dicts = [get_tag_sum_count_from_file(f'tag_nsteps_{i}.csv') for i in range(1,11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выведите на экран значение по ключу \"30-minutes-or-less\" для каждого из словарей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sum': 278749, 'count': 36400},\n",
       " {'sum': 275881, 'count': 36302},\n",
       " {'sum': 283843, 'count': 37125},\n",
       " {'sum': 277321, 'count': 36547},\n",
       " {'sum': 278753, 'count': 36434},\n",
       " {'sum': 275007, 'count': 36251},\n",
       " {'sum': 278495, 'count': 36595},\n",
       " {'sum': 278785, 'count': 36692},\n",
       " {'sum': 277874, 'count': 36652},\n",
       " {'sum': 278497, 'count': 36784}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d['30-minutes-or-less'] for d in list_of_dicts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Напишите функцию, которая объединяет результаты обработки отдельных файлов. Данная функция принимает на вход список словарей, каждый из которых является результатом вызова функции `get_tag_sum_count_from_file` для конкретного файла, и агрегирует эти словари. Не используйте параллельных вычислений.\n",
    "\n",
    "Процедура агрегации словарей имеет следующий вид:\n",
    "$$d_{agg}[k] = \\{sum: \\sum_{i=1}^{n}d_{i}[k][sum], count: \\sum_{i=1}^{n}d_{i}[k][count]\\}$$\n",
    "где $d_1, d_2, ..., d_n$- результат вызова функции `get_tag_sum_count_from_file` для конкретных файлов.\n",
    "\n",
    "Примените данную функцию к результату выполнения задания 2. Выведите на экран результат для тэга \"30-minutes-or-less\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_results(tag_sum_count_list: list) -> dict:\n",
    "    df = pd.DataFrame(tag_sum_count_list)\n",
    "    dict_agg = {tag: {'sum':0, 'count':0} for tag in list(set(df.columns))}\n",
    "    for file in list_of_dicts:\n",
    "        for tag in file:\n",
    "            dict_agg[tag]['sum'] += file[tag]['sum']\n",
    "            dict_agg[tag]['count'] += file[tag]['count'] \n",
    "    return dict_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_sum_count = agg_results(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sum': 2783205, 'count': 365782}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_sum_count['30-minutes-or-less']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Напишите функцию, которая считает среднее значение количества шагов для каждого тэга в словаре, имеющего вид, аналогичный словарям в задаче 2, и возвращает результат в виде словаря . Используйте решения задач 1-3, чтобы получить среднее значение количества шагов каждого тэга для всего датасета, имея результаты обработки частей датасета и результат их агрегации. Выведите на экран результат для тэга \"30-minutes-or-less\".\n",
    "\n",
    "Определите, за какое время задача решается для всего датасета. При замере времени учитывайте время расчета статистики для каждого файла, агрегации результатов и, собственно, вычисления средного. Временем, затрачиваемым на процедуру разбиения исходного файла можно пренебречь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_mean_n_steps(tag_sum_count: dict) -> dict:\n",
    "    df = pd.DataFrame(tag_sum_count).T\n",
    "    df['mean'] = df['sum'] / df['count']\n",
    "    df.drop(['sum','count'],axis=1,inplace=True)\n",
    "    return df.T.to_dict('dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 7.608917333275011}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tag_mean_n_steps(tag_sum_count)['30-minutes-or-less']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.8 s ± 2.58 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "get_tag_mean_n_steps(agg_results([get_tag_sum_count_from_file(f'tag_nsteps_{i}.csv') for i in range(1,11)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Повторите решение задачи 4, распараллелив вызовы функции `get_tag_sum_count_from_file` для различных файлов с помощью `multiprocessing.Pool`. Для обработки каждого файла создайте свой собственный процесс. Выведите на экран результат для тэга \"30-minutes-or-less\". Определите, за какое время задача решается для всех файлов. При замере времени учитывайте время расчета статистики для каждого файла, агрегации результатов и, собственно, вычисления средного. Временем, затрачиваемым на процедуру разбиения исходного файла можно пренебречь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f'tag_nsteps_{i}.csv' for i in range(1,11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_tag_sum_count_from_file.py\n"
     ]
    }
   ],
   "source": [
    "%%file get_tag_sum_count_from_file.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_tag_sum_count_from_file(file: str) -> dict:\n",
    "    df = pd.read_csv(file, dtype={'tags': 'str', 'n_steps': np.int64})\n",
    "    df = df.groupby('tags') \\\n",
    "        .agg({'n_steps':'sum', 'tags':'count'}) \\\n",
    "        .rename(columns={'n_steps':'sum','tags':'count'})\n",
    "    return df.T.to_dict('dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_tag_sum_count_from_file import get_tag_sum_count_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 7.608917333275011}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with multiprocessing.Pool(processes=10) as pool:\n",
    "    res = pool.map(get_tag_sum_count_from_file, files)\n",
    "    \n",
    "get_tag_mean_n_steps(agg_results(res))['30-minutes-or-less']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.42 s ± 146 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with multiprocessing.Pool(processes=10) as pool:\n",
    "    res = pool.map(get_tag_sum_count_from_file, files)\n",
    "    \n",
    "get_tag_mean_n_steps(agg_results(res)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Повторите решение задачи 4, распараллелив вычисления функции `get_tag_sum_count_from_file` для различных файлов с помощью `multiprocessing.Process`. Для обработки каждого файла создайте свой собственный процесс. Для обмена данными между процессами используйте `multiprocessing.Queue`.\n",
    "\n",
    "Выведите на экран результат для тэга \"30-minutes-or-less\". Определите, за какое время задача решается для всех файлов. При замере времени учитывайте время расчета статистики для каждого файла, агрегации результатов и, собственно, вычисления средного. Временем, затрачиваемым на процедуру разбиения исходного файла можно пренебречь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing get_tag_sum_count_from_file_q.py\n"
     ]
    }
   ],
   "source": [
    "%%file get_tag_sum_count_from_file_q.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_tag_sum_count_from_file_q(file: str, queue) -> dict:\n",
    "    df = pd.read_csv(file, dtype={'tags': 'str', 'n_steps': np.int64})\n",
    "    df = df.groupby('tags') \\\n",
    "        .agg({'n_steps':'sum', 'tags':'count'}) \\\n",
    "        .rename(columns={'n_steps':'sum','tags':'count'})\n",
    "    return queue.put(df.T.to_dict('dict'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_tag_sum_count_from_file_q import get_tag_sum_count_from_file_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 7.608917333275011}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = []\n",
    "queue = multiprocessing.Queue()\n",
    "\n",
    "for f in files:\n",
    "    p = multiprocessing.Process(target=get_tag_sum_count_from_file_q, args=(f, queue))\n",
    "    ps.append(p)\n",
    "    p.start()\n",
    "\n",
    "rs = []\n",
    "while len(rs) < 10:\n",
    "    if not queue.empty():\n",
    "        rs.append(queue.get())\n",
    "    \n",
    "for p in ps:\n",
    "    p.join()\n",
    "\n",
    "get_tag_mean_n_steps(agg_results(rs))['30-minutes-or-less']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.39 s ± 79.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "ps = []\n",
    "queue = multiprocessing.Queue()\n",
    "\n",
    "for f in files:\n",
    "    p = multiprocessing.Process(target=get_tag_sum_count_from_file_q, args=(f, queue))\n",
    "    ps.append(p)\n",
    "    p.start()\n",
    "\n",
    "rs = []\n",
    "while len(rs) < 10:\n",
    "    if not queue.empty():\n",
    "        rs.append(queue.get())\n",
    "    \n",
    "for p in ps:\n",
    "    p.join()\n",
    "\n",
    "get_tag_mean_n_steps(agg_results(rs)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Исследуйте, как влияет количество запущенных одновременно процессов на скорость решения задачи. Узнайте количество ядер вашего процессора $K$. Повторите решение задачи 1, разбив исходный файл на $\\frac{K}{2}$, $K$ и $2K$ фрагментов. Для каждого из разбиений повторите решение задачи 5. Визуализируйте зависимость времени выполнения кода от количества файлов в разбиении. Сделайте вывод в виде текстового комментария."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Напишите функцию `parallel_map`, которая принимает на вход серию `s` `pd.Series` и функцию одного аргумента `f` и поэлементно применяет эту функцию к серии, распараллелив вычисления при помощи пакета `multiprocessing`. Логика работы функции `parallel_map` должна включать следующие действия:\n",
    "* разбиение исходной серии на $K$ частей, где $K$ - количество ядер вашего процессора;\n",
    "* параллельное применение функции `f` к каждой части при помощи метода _серии_ `map` при помощи нескольких подпроцессов;\n",
    "* объединение результатов работы подпроцессов в одну серию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_map(s: pd.Series, f: callable) -> pd.Series:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Напишите функцию `f`, которая принимает на вход тэг и проверяет, удовлетворяет ли тэг следующему шаблону: `[любое число]-[любое слово]-or-less`. Возьмите любой фрагмент файла, полученный в задании 1, примените функцию `f` при помощи `parallel_map` к столбцу `tags` и посчитайте количество тэгов, подходящих под этот шаблон. Решите ту же задачу, воспользовавшись методом _серий_ `map`. Сравните время и результат выполнения двух решений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(tag: str) -> bool:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Используя пакет `pandarallel`, примените функцию `f` из задания 9 к столбцу `tags` таблицы, с которой вы работали этом задании. Посчитайте количество тэгов, подходящих под описанный шаблон. Измерьте время выполнения кода. Выведите на экран полученный результат."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
